services:

  # ollama:
  #   container_name: ollama
  #   image: ollama/ollama:latest
  #   ports:
  #     - 11434:11434
  #   volumes:
  #     - $OLLAMA_MODEL_DIR:/root/.ollama
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             capabilities: [ gpu ]
  #             device_ids: [ $GPU_IDX ]

  api:
    # image: ghcr.io/jasonlo/text2graph_llm_api:latest
    build:
      dockerfile: ./api/Dockerfile
    ports:
      - $API_PORT:4502
    env_file: .env

  demo:
    # image: ghcr.io/jasonlo/text2graph_llm_demo:latest
    build:
      dockerfile: ./demo/Dockerfile
    ports:
      - $DEMO_PORT:8501
    env_file: .env

  chtc-debug:
    build:
      dockerfile: ./chtc/Dockerfile
    stdin_open: true
    tty: true
    env_file: ./chtc/.env
    volumes:
      - ./working.ipynb:/root/working.ipynb
      - ./chtc:/root/chtc
      - ./.cache:/root/.cache
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [ gpu ]
              device_ids: [ "6" ]
