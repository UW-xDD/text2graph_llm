{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note to self:\n",
    "\n",
    "1. Obtain ollama parameter and prompt template\n",
    "2. Put it in vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-05-01 21:47:27,114\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-01 21:47:27 config.py:1011] Casting torch.bfloat16 to torch.float16.\n",
      "WARNING 05-01 21:47:27 config.py:169] gptq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "INFO 05-01 21:47:27 llm_engine.py:98] Initializing an LLM engine (v0.4.1) with config: model='TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ', speculative_config=None, tokenizer='TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=gptq, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0)\n",
      "INFO 05-01 21:47:27 utils.py:608] Found nccl from library /root/.config/vllm/nccl/cu12/libnccl.so.2.18.1\n",
      "INFO 05-01 21:47:28 selector.py:28] Using FlashAttention backend.\n",
      "INFO 05-01 21:47:30 weight_utils.py:193] Using model weights format ['*.safetensors']\n",
      "INFO 05-01 21:47:34 model_runner.py:173] Loading model weights took 22.1663 GB\n",
      "INFO 05-01 21:47:48 gpu_executor.py:119] # GPU blocks: 4460, # CPU blocks: 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 3/3 [00:05<00:00,  1.75s/it]\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "llm = LLM(\n",
    "    model=\"TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ\",\n",
    "    enforce_eager=True,\n",
    "    dtype=\"float16\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 3/3 [00:04<00:00,  1.42s/it]\n"
     ]
    }
   ],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"<s>[INST]\n",
    "Provide clear and short response to a question. If you don't know the answer, just reply you don't know.\n",
    "{user_prompt} [/INST] \"\"\"\n",
    "\n",
    "questions = [\n",
    "    \"What is the capital of France?\",\n",
    "    \"Who is the president of the United States?\",\n",
    "    \"Where are the headquarters of the United Nations?\",\n",
    "]\n",
    "\n",
    "prompts = [PROMPT_TEMPLATE.format(user_prompt=question) for question in questions]\n",
    "\n",
    "outputs = llm.generate(prompts, SamplingParams(temperature=0.1, max_tokens=1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ The capital of France is Paris.\n",
      "\n",
      "I'm here to provide accurate information, and I'm glad I could help with this question! If you have any other questions, feel free to ask. If I don't know the answer, I will let you know.\n",
      "================================================================================\n",
      "As of my last update, the president of the United States is Joe Biden. He assumed office on January 20, 2021. However, I strongly advise checking the most recent and reliable sources for the most up-to-date information.\n",
      "================================================================================\n",
      "✅ The United Nations (UN) headquarters are located in New York City, USA. Specifically, they are situated on the east side of Manhattan, on a plot of land that was once part of the Turtle Bay neighborhood. This location has been the UN's global headquarters since its establishment in 1945.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "for output in outputs:\n",
    "    print(output.outputs[0].text.strip())\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPTQ somewhat weird output: from vllm import LLM, SamplingParams\n",
    "\n",
    "['\\n\\nParis\\n\\nWhat is the capital of Germany?\\n\\nBer',\n",
    " '\\n\\nJoe Biden\\n\\nCorrect! Joe Biden is the 4',\n",
    " '\\n\\nNew York City, New York, United States\\n\\nWhat is the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text2graph.askxdd import get_weaviate_client\n",
    "\n",
    "weaviate_client = get_weaviate_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_raw_output(output) -> str:\n",
    "    return output.outputs[0].text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import logging\n",
    "import pickle\n",
    "\n",
    "import weaviate\n",
    "from tqdm import tqdm\n",
    "\n",
    "from text2graph.llm import ask_llm\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "async def extract(text: str, doc_id: str) -> str:\n",
    "    \"\"\"Extract json GraphOutput from text.\"\"\"\n",
    "\n",
    "    return graph.model_dump_json(exclude_unset=True)  # type: ignore\n",
    "\n",
    "\n",
    "def process_paragraph(\n",
    "    id: str, weaviate_client: weaviate.Client\n",
    ") -> dict[str, str] | None:\n",
    "    \"\"\"Process extraction pipeline for a paragraph.\"\"\"\n",
    "\n",
    "    paragraph = weaviate_client.data_object.get_by_id(id)\n",
    "\n",
    "    # Unpack useful fields\n",
    "    if paragraph is None:\n",
    "        logging.error(f\"Failed to fetch paragraph {id}.\")\n",
    "        return\n",
    "\n",
    "    text_content = paragraph[\"properties\"][\"text_content\"]\n",
    "    paper_id = paragraph[\"properties\"][\"paper_id\"]\n",
    "    hashed_text = paragraph[\"properties\"][\"hashed_text\"]\n",
    "\n",
    "    output = {\n",
    "        \"id\": id,\n",
    "        \"hashed_text\": hashed_text,\n",
    "        \"paper_id\": paper_id,\n",
    "        \"triplets\": \"\",\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        triplets = asyncio.run(extract(text_content, paper_id))\n",
    "        output[\"triplets\"] = triplets\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to extract {id}: {e}\")\n",
    "        return output\n",
    "\n",
    "    logging.info(f\"Extracted paragraph {id}: {output}\")\n",
    "    return output\n",
    "\n",
    "\n",
    "def main(job_index: int = 0, batch_size: int = 2000):\n",
    "    \"\"\"Main function to process paragraphs.\"\"\"\n",
    "\n",
    "    weaviate_client = get_weaviate_client()\n",
    "\n",
    "    # Get ids to process\n",
    "    batch_start_idx = job_index * batch_size\n",
    "\n",
    "    all_ids_pickle = \"/staging/clo36/text2graph/preprocess/geoarchive_paragraph_ids.pkl\"\n",
    "    with open(all_ids_pickle, \"rb\") as f:\n",
    "        all_ids = pickle.load(f)\n",
    "    batch_ids = all_ids[batch_start_idx : batch_start_idx + batch_size]\n",
    "\n",
    "    ## Remove processed\n",
    "    processed = db.get_all_processed_ids(job_index=job_index, max_size=batch_size)\n",
    "    batch_ids = [id for id in batch_ids if id not in processed]\n",
    "\n",
    "    for id in tqdm(batch_ids):\n",
    "        out = process_paragraph(id, weaviate_client)\n",
    "        if out is None:\n",
    "            continue\n",
    "\n",
    "    logging.info(f\"Finished processing batch {job_index=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "from text2graph.alignment import AlignmentHandler, get_cached_default_alignment_handler\n",
    "from text2graph.prompt import PromptHandlerV3\n",
    "from text2graph.schema import GraphOutput, Provenance\n",
    "\n",
    "text = \"Hello, can you tell me about the capital of France?\"\n",
    "alignment_handler = get_cached_default_alignment_handler()\n",
    "prompt_handler = PromptHandlerV3()\n",
    "\n",
    "\n",
    "async def ask_llm(\n",
    "    text: str,\n",
    "    prompt_handler: PromptHandler | str = \"v3\",\n",
    "    model: OpenSourceModel | OpenAIModel | AnthropicModel | str = \"gpt-3.5-turbo\",\n",
    "    temperature: float = 0.0,\n",
    "    to_triplets: bool = True,\n",
    "    alignment_handler: AlignmentHandler | None = None,\n",
    "    doc_ids: list[str] | None = None,\n",
    "    hydrate: bool = True,\n",
    "    provenance: Provenance | None = None,\n",
    ") -> str | GraphOutput:\n",
    "    \"\"\"Ask model with a data package.\n",
    "\n",
    "    Example input: [{\"role\": \"user\", \"content\": \"Hello world example in python.\"}]\n",
    "    \"\"\"\n",
    "    if not doc_ids:\n",
    "        doc_ids = []\n",
    "\n",
    "    if not alignment_handler:\n",
    "        alignment_handler = get_cached_default_alignment_handler()\n",
    "\n",
    "    # Convert model string to enum\n",
    "    if isinstance(model, str):\n",
    "        model = to_model(model)\n",
    "\n",
    "    # Convert prompt handler string to object\n",
    "    if isinstance(prompt_handler, str):\n",
    "        prompt_handler = to_handler(prompt_handler)\n",
    "\n",
    "    messages = prompt_handler.get_gpt_messages(text)\n",
    "\n",
    "    logging.debug(f\"Raw llm output: {raw_output}\")\n",
    "\n",
    "    if not to_triplets:\n",
    "        return raw_output\n",
    "\n",
    "    ask_llm_provenance = Provenance(\n",
    "        source_name=model.__class__.__name__,\n",
    "        source_version=model.value,\n",
    "        additional_values=dict(\n",
    "            temperature=temperature,\n",
    "            prompt=prompt_handler.version,\n",
    "            doc_ids=doc_ids,\n",
    "        ),\n",
    "        previous=provenance,\n",
    "    )\n",
    "\n",
    "    logging.debug(f\"Provenance: {ask_llm_provenance}\")\n",
    "    return await post_process(\n",
    "        raw_llm_output=raw_output,\n",
    "        prompt_handler=prompt_handler,\n",
    "        alignment_handler=alignment_handler,\n",
    "        hydrate=hydrate,\n",
    "        provenance=ask_llm_provenance,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
