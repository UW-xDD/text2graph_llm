{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity alignment experiment\n",
    "\n",
    "Problem: The entities extracted from LLM can be messy, we need to align it into some canonical form to make sure they are referring to the same thing.\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Extract entities\n",
    "2. Project to semantic space\n",
    "3. Use distance metric to defines it canonical form, semi-manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "import tenacity\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "load_dotenv()\n",
    "CHTC_LLM_HOST = os.getenv('CHTC_LLM_HOST', \"\")\n",
    "CHTC_LLM_PORT = os.getenv('CHTC_LLM_PORT', \"\")\n",
    "CHTC_LLM_API_KEY = os.getenv('CHTC_LLM_API_KEY', \"\")\n",
    "CHTC_LLM_API_URL = f\"http://{CHTC_LLM_HOST}:{CHTC_LLM_PORT}\"\n",
    "\n",
    "\n",
    "BATCH_SIZE = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if LLM endpoint is running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "requests.get(CHTC_LLM_API_URL).json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic LLM extraction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tenacity.retry(wait=tenacity.wait_fixed(5), stop=tenacity.stop_after_attempt(5))\n",
    "def extract(text: str) -> dict:\n",
    "    \"\"\"Extract entities from text.\"\"\"\n",
    "\n",
    "    auth_headers = {\"Api-Key\": CHTC_LLM_API_KEY}\n",
    "    payload = {\n",
    "        \"model\": \"my_model\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"Try to extract all locations, stratigraphic names, lithologies from the text provided. Reply in JSON format with the following structure: {\\\"locations:\\\": \\\"\\\", \\\"stratigraphic_names\\\": \\\"\\\", \\\"lithologies\\\": \\\"\\\"}. If you can't find any of the requested information, leave the corresponding field empty.Do not include any other information in the response.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": text\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    "    response = requests.post(f\"{CHTC_LLM_API_URL}/v1/chat/completions\", headers=auth_headers, json=payload)\n",
    "    response.raise_for_status()\n",
    "    content = json.loads(response.json()['_content'])\n",
    "    return json.loads(content[\"choices\"][0]['message']['content'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"In the rolling hills of the Devonshire region, geologists have identified a fascinating stratigraphic layer known as the Devonian Slate. This stratum, rich in history and significance, dates back to the Devonian period, showcasing a deep, grayish-black coloration that speaks to its volcanic ash origin. The lithology of the Devonian Slate is particularly noteworthy; it exhibits a fine-grained texture with a smooth to slightly fissile surface, indicating a high degree of compaction and low-grade metamorphism over millions of years. Embedded within this slate, one can find intermittent layers of quartz and small fossils, hinting at the dynamic environmental conditions that prevailed during its formation. The study of such layers not only enriches our understanding of Earth's geological past but also provides valuable insights into the processes that have shaped the planet's crust over aeons.\"\n",
    "\n",
    "extract(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a pipeline to process the criticalMAAS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate\n",
    "\n",
    "WEAVIATE_APIKEY = os.getenv(\"WEAVIATE_APIKEY\")\n",
    "WEAVIATE_URL = os.getenv(\"WEAVIATE_URL\")\n",
    "\n",
    "client = weaviate.Client(WEAVIATE_URL, weaviate.AuthApiKey(api_key=WEAVIATE_APIKEY))\n",
    "\n",
    "def get_batch_with_cursor(class_properties: list[str], class_name:str=\"Paragraph\", batch_size:int=BATCH_SIZE, offset:int | None=None):\n",
    "\n",
    "    if \"topic_list\" not in class_properties:\n",
    "        class_properties.append(\"topic_list\")\n",
    "    query = (\n",
    "        client.query.get(class_name, class_properties)\n",
    "        .with_additional([\"id\"])\n",
    "        .with_where({\"path\": \"topic_list\", \"operator\": \"ContainsAny\", \"valueText\": [\"criticalmaas\"]})\n",
    "    )\n",
    "\n",
    "    if offset is not None:\n",
    "        query = query.with_offset(offset)\n",
    "\n",
    "    return query.with_limit(batch_size).do()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a local DB to store results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_case(hashed_text:str, paper_id:str, locations:str, stratigraphic_names:str, lithologies:str) -> None:\n",
    "    with sqlite3.connect(\"entities.db\") as conn:\n",
    "        cur = conn.cursor()\n",
    "        cur.execute('''\n",
    "        INSERT INTO entities (hashed_text, paper_id, locations, stratigraphic_names, lithologies)\n",
    "        VALUES (?, ?, ?, ?, ?)\n",
    "        ''', (hashed_text, paper_id, locations, stratigraphic_names, lithologies))\n",
    "        conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_db(hashed_text: str) -> bool:\n",
    "    with sqlite3.connect(\"entities.db\") as conn:\n",
    "        cur = conn.cursor()\n",
    "        cur.execute(\"SELECT hashed_text FROM entities WHERE hashed_text=?\", (hashed_text,))\n",
    "        result = cur.fetchone()\n",
    "    return result is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_case(paragraph: dict) -> None:\n",
    "\n",
    "    # Check if the paragraph has already been processed\n",
    "    if in_db(paragraph[\"hashed_text\"]):\n",
    "        logging.info(f\"Paragraph {paragraph['hashed_text']} already processed.\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        entities = extract(paragraph[\"text_content\"])\n",
    "    except tenacity.RetryError:\n",
    "        logging.error(f\"Failed to extract entities for paragraph {paragraph['hashed_text']}.\")\n",
    "        return\n",
    "    entities[\"hashed_text\"] = paragraph[\"hashed_text\"]\n",
    "    entities[\"paper_id\"] = paragraph[\"paper_id\"]\n",
    "\n",
    "    logging.info(f\"Extracted entities for paragraph {paragraph['hashed_text']}: {entities}\")\n",
    "    insert_case(**entities)\n",
    "    logging.info(f\"Inserted entities for paragraph {paragraph['hashed_text']} into the database.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
