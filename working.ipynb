{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark\n",
    "\n",
    "1. It should only evaluate the LLM engine speed, but not other unrelated pre/post process\n",
    "2. Metric of choice: Token per second (TPS)\n",
    "3. Varying parameter: batch_size, tensor_parallel_size, enforce_eager\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import time\n",
    "\n",
    "from text2graph.prompt import PromptHandlerV3\n",
    "from text2graph.askxdd import get_weaviate_client\n",
    "\n",
    "\n",
    "import vllm\n",
    "import os\n",
    "\n",
    "os.chdir(\"/root\")\n",
    "\n",
    "from chtc.db import Triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = vllm.LLM(\n",
    "    model=\"TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ\",\n",
    "    dtype=\"float16\",\n",
    "    tensor_parallel_size=1,\n",
    "    enforce_eager=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(batch_size: int) -> list[str]:\n",
    "    \"\"\"Get a batch of prompts for benchmarking.\"\"\"\n",
    "\n",
    "    weaviate_client = get_weaviate_client()\n",
    "    prompt_handler = PromptHandlerV3()\n",
    "    MIXTRAL_TEMPLATE = \"[INST] {system} {user} [/INST]\"\n",
    "\n",
    "    # Get ids\n",
    "    all_ids_pickle = \"/root/geoarchive_paragraph_ids.pkl\"\n",
    "    with open(all_ids_pickle, \"rb\") as f:\n",
    "        all_ids = pickle.load(f)\n",
    "    batch_ids = all_ids[0:batch_size]\n",
    "\n",
    "    batch = []\n",
    "    for id in batch_ids:\n",
    "        paragraph = weaviate_client.data_object.get_by_id(id, class_name=\"Paragraph\")\n",
    "        text = paragraph[\"properties\"][\"text_content\"]\n",
    "        messages = prompt_handler.get_gpt_messages(text)\n",
    "        prompt = MIXTRAL_TEMPLATE.format(\n",
    "            system=messages[0][\"content\"], user=messages[1][\"content\"]\n",
    "        )\n",
    "        batch.append(prompt)\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(batch_size: int, llm: vllm.LLM) -> dict:\n",
    "    \"\"\"Benchmark the time required to process the llm inference.\"\"\"\n",
    "\n",
    "    sampling_params = vllm.SamplingParams(\n",
    "        temperature=0, max_tokens=2048, stop=[\"[/INST]\", \"[INST]\"]\n",
    "    )\n",
    "    prompts = get_batch(batch_size)\n",
    "\n",
    "    # Timed section\n",
    "    start_time = time.perf_counter()\n",
    "    outputs = llm.generate(prompts, sampling_params)\n",
    "    end_time = time.perf_counter()\n",
    "\n",
    "    # Calculate time per token\n",
    "    results = {}\n",
    "    results[\"total_time\"] = end_time - start_time\n",
    "    results[\"input_tokens\"] = sum([len(output.prompt_token_ids) for output in outputs])\n",
    "    results[\"output_tokens\"] = sum(\n",
    "        [len(output.outputs[0].token_ids) for output in outputs]\n",
    "    )\n",
    "    results[\"token_per_second\"] = (\n",
    "        results[\"input_tokens\"] + results[\"output_tokens\"]\n",
    "    ) / results[\"total_time\"]\n",
    "    results[\"outputs\"] = outputs\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for batch_size in [3]:\n",
    "    results[batch_size] = benchmark(batch_size, llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_size, r in results.items():\n",
    "    data = {k: v for k, v in r.items() if k != \"outputs\"}\n",
    "    print(f\"Batch size: {batch_size}, {data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tp: 2; eager: False, OOM at 50\n",
    "\n",
    "- Batch size: 1, {'total_time': 4.926733806729317, 'input_tokens': 369, 'output_tokens': 232, 'token_per_second': 121.98751212803651}\n",
    "- Batch size: 3, {'total_time': 15.417909558862448, 'input_tokens': 1122, 'output_tokens': 1098, 'token_per_second': 143.9883916509233}\n",
    "- Batch size: 5, {'total_time': 22.074966586660594, 'input_tokens': 1897, 'output_tokens': 2134, 'token_per_second': 182.60503290799284}\n",
    "- Batch size: 10, {'total_time': 26.61369479680434, 'input_tokens': 3807, 'output_tokens': 3494, 'token_per_second': 274.33244634926353}\n",
    "- Batch size: 20, {'total_time': 43.88761671539396, 'input_tokens': 7865, 'output_tokens': 7980, 'token_per_second': 361.03578152245}\n",
    "\n",
    "tp: 1; eager: True\n",
    "\n",
    "- Batch size: 1, {'total_time': 12.907626039814204, 'input_tokens': 369, 'output_tokens': 211, 'token_per_second': 44.93467646265562}\n",
    "- Batch size: 3, {'total_time': 52.06450440501794, 'input_tokens': 1122, 'output_tokens': 1161, 'token_per_second': 43.849452253308414}\n",
    "- Batch size: 5, {'total_time': 42.428753749933094, 'input_tokens': 1897, 'output_tokens': 2186, 'token_per_second': 96.23190971067442}\n",
    "- Batch size: 10, {'total_time': 49.54290740704164, 'input_tokens': 3807, 'output_tokens': 3833, 'token_per_second': 154.20976280681722}\n",
    "- Batch size: 20, {'total_time': 74.78418159391731, 'input_tokens': 7865, 'output_tokens': 6638, 'token_per_second': 193.9313861686978}\n",
    "- Batch size: 50, {'total_time': 172.57586133107543, 'input_tokens': 20854, 'output_tokens': 21304, 'token_per_second': 244.2867714802979}\n",
    "- Batch size: 100, {'total_time': 205.62514622602612, 'input_tokens': 40934, 'output_tokens': 40364, 'token_per_second': 395.36993160669203}\n",
    "- Batch size: 200, {'total_time': 293.219068787992, 'input_tokens': 82229, 'output_tokens': 74914, 'token_per_second': 535.923535428796}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import logging\n",
    "import asyncio\n",
    "from text2graph.prompt import PromptHandlerV3, PromptHandler\n",
    "from text2graph.alignment import AlignmentHandler\n",
    "from text2graph.askxdd import get_weaviate_client\n",
    "from text2graph.llm import post_process\n",
    "from text2graph.schema import Provenance\n",
    "import vllm\n",
    "import os\n",
    "\n",
    "os.chdir(\"/root\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chtc.db import Triplets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process_with_prov(\n",
    "    ids: list[str],\n",
    "    paper_ids: list[str],\n",
    "    hashed_texts: list[str],\n",
    "    raw_outputs: list[str],\n",
    "    llm: vllm.LLM,\n",
    "    prompt_handler: PromptHandler,\n",
    "    alignment_handler: AlignmentHandler,\n",
    "    sampling_params: vllm.SamplingParams,\n",
    ") -> list[dict]:\n",
    "    \"\"\"Post processing with provenance.\"\"\"\n",
    "\n",
    "    outputs = []\n",
    "    for id, paper_id, hashed_text, raw_output in zip(\n",
    "        ids, paper_ids, hashed_texts, raw_outputs\n",
    "    ):\n",
    "        vllm_prov = Provenance(\n",
    "            source_name=\"vllm\",\n",
    "            source_version=llm.llm_engine.model_config.__dict__[\"model\"],\n",
    "            additional_values={\n",
    "                \"temperature\": sampling_params.temperature,\n",
    "                \"paragraph_id\": id,\n",
    "                \"doc_ids\": [paper_id],\n",
    "            },\n",
    "        )\n",
    "        try:\n",
    "            triplets = asyncio.run(\n",
    "                post_process(\n",
    "                    raw_output,\n",
    "                    prompt_handler,\n",
    "                    alignment_handler,\n",
    "                    hydrate=False,\n",
    "                    provenance=vllm_prov,\n",
    "                )\n",
    "            )\n",
    "            this_output = {\n",
    "                \"id\": id,\n",
    "                \"hashed_text\": hashed_text,\n",
    "                \"paper_id\": paper_id,\n",
    "                \"triplets\": triplets,\n",
    "            }\n",
    "            outputs.append(this_output)\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error post-processing paragraph {id}: {e}\")\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def main(\n",
    "    job_index: int, batch_size: int = 2000, mini_batch_size: int = 200\n",
    ") -> list[str]:\n",
    "    \"\"\"Get a batch of prompts for benchmarking.\"\"\"\n",
    "\n",
    "    # Infrastructure\n",
    "    weaviate_client = get_weaviate_client()\n",
    "    prompt_handler = PromptHandlerV3()\n",
    "    llm = vllm.LLM(\n",
    "        model=\"TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ\",\n",
    "        dtype=\"float16\",\n",
    "        tensor_parallel_size=1,\n",
    "        enforce_eager=True,\n",
    "    )\n",
    "    alignment_handler = AlignmentHandler.load()\n",
    "\n",
    "    # Mixtral settings\n",
    "    MIXTRAL_TEMPLATE = \"[INST] {system} {user} [/INST]\"\n",
    "    sampling_params = vllm.SamplingParams(\n",
    "        temperature=0, max_tokens=2048, stop=[\"[/INST]\", \"[INST]\"]\n",
    "    )\n",
    "\n",
    "    # Get batch_ids\n",
    "    all_ids_pickle = \"/root/geoarchive_paragraph_ids.pkl\"\n",
    "    with open(all_ids_pickle, \"rb\") as f:\n",
    "        all_ids = pickle.load(f)\n",
    "    batch_ids = all_ids[0:batch_size]\n",
    "\n",
    "    # Process a mini batch\n",
    "    def process_mini_batch(ids: list[str]) -> list[dict]:\n",
    "        \"\"\"Process a batch of ids.\"\"\"\n",
    "\n",
    "        # Get prompts and metadata\n",
    "        hashed_texts, paper_ids, prompts = [], [], []\n",
    "\n",
    "        for id in ids:\n",
    "            paragraph = weaviate_client.data_object.get_by_id(\n",
    "                id, class_name=\"Paragraph\"\n",
    "            )\n",
    "            text = paragraph[\"properties\"][\"text_content\"]\n",
    "            messages = prompt_handler.get_gpt_messages(text)\n",
    "            prompt = MIXTRAL_TEMPLATE.format(\n",
    "                system=messages[0][\"content\"], user=messages[1][\"content\"]\n",
    "            )\n",
    "            prompts.append(prompt)\n",
    "            hashed_texts.append(paragraph[\"properties\"][\"hashed_text\"])\n",
    "            paper_ids.append(paragraph[\"properties\"][\"paper_id\"])\n",
    "\n",
    "        # Generate LLM outputs\n",
    "        llm_outputs = llm.generate(prompts, sampling_params)\n",
    "        raw_outputs = [output.outputs[0].text.strip() for output in llm_outputs]\n",
    "\n",
    "        # Post-process\n",
    "        outputs = post_process_with_prov(\n",
    "            ids, paper_ids, hashed_texts, raw_outputs, llm, prompt_handler, alignment_handler, sampling_params\n",
    "        )\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    # Mini-batching\n",
    "    db_objects = []\n",
    "    while len(batch_ids) > 0:\n",
    "        n_in_batch = min(mini_batch_size, len(batch_ids))\n",
    "        mini_batch_ids = [batch_ids.pop() for _ in range(n_in_batch)]\n",
    "        outputs = process_mini_batch(mini_batch_ids)\n",
    "        db_objects.extend([Triplets(**output, job_id=job_index) for output in outputs])\n",
    "\n",
    "    return db_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "y = main(job_index=0, batch_size=20, mini_batch_size=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " {\"triplets\": [{\"location\": \"blocks interpolated in runs 1 and 2\",\"relationship\": \"categorized as\",\"stratigraphic_name\": \"indicated category\"},{\"location\": \"blocks interpolated in runs 3 and 4\",\"relationship\": \"categorized as\",\"stratigraphic_name\": \"inferred category\"},{\"location\": \"blocks interpolated in run 5\",\"relationship\": \"dropped from\",\"stratigraphic_name\": \"resource pool\"},{\"location\": \"blocks estimated above 1.5 g/t Au\",\"relationship\": \"captured\",\"stratigraphic_name\": \"reported resources\"},{\"location\": \"blocks estimated above 1.5 g/t Au\",\"relationship\": \"capture\",\"stratigraphic_name\": \"98% of the ounces at no cut-off\"},{\"location\": \"blocks estimated above 1.5 g/t Au\",\"relationship\": \"have\",\"stratigraphic_name\": \"an average grade about 11% higher i.e. 5.7 g/t\"}]}\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    " {\"triplets\": [{\"location\": \"blocks interpolated in runs 1 and 2\",\"relationship\": \"categorized as\",\"stratigraphic_name\": \"indicated category\"},{\"location\": \"blocks interpolated in runs 3 and 4\",\"relationship\": \"categorized as\",\"stratigraphic_name\": \"inferred category\"},{\"location\": \"blocks interpolated in run 5\",\"relationship\": \"dropped from\",\"stratigraphic_name\": \"resource pool\"},{\"location\": \"blocks estimated above 1.5 g/t Au\",\"relationship\": \"captured\",\"stratigraphic_name\": \"reported resources\"},{\"location\": \"blocks estimated above 1.5 g/t Au\",\"relationship\": \"capture\",\"stratigraphic_name\": \"98% of the ounces at no cut-off\"},{\"location\": \"blocks estimated above 1.5 g/t Au\",\"relationship\": \"have\",\"stratigraphic_name\": \"an average grade about 11% higher i.e. 5.7 g/t\"}]}Note: The term \"stratigraphic_name\" is used to represent any term that could be a stratigraphic name, as the actual stratigraphic names in the text are not provided. The user should replace this term with the actual stratigraphic names.\n",
    "\"\"\"\n",
    "import re\n",
    "# find last \"{\" and remove everything after it\n",
    "cleaned_text = re.sub(r'\\}[^}]*$', '}', text)\n",
    "\n",
    "print(cleaned_text)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
