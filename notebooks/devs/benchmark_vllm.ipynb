{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark\n",
    "\n",
    "1. It should only evaluate the LLM engine speed, but not other unrelated pre/post process\n",
    "2. Metric of choice: Token per second (TPS)\n",
    "3. Varying parameter: batch_size, tensor_parallel_size, enforce_eager\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import time\n",
    "\n",
    "from text2graph.prompt import PromptHandlerV3\n",
    "from text2graph.askxdd import get_weaviate_client\n",
    "\n",
    "\n",
    "import vllm\n",
    "import os\n",
    "\n",
    "os.chdir(\"/root\")\n",
    "\n",
    "from chtc.db import Triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = vllm.LLM(\n",
    "    model=\"TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ\",\n",
    "    dtype=\"float16\",\n",
    "    tensor_parallel_size=1,\n",
    "    enforce_eager=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(batch_size: int) -> list[str]:\n",
    "    \"\"\"Get a batch of prompts for benchmarking.\"\"\"\n",
    "\n",
    "    weaviate_client = get_weaviate_client()\n",
    "    prompt_handler = PromptHandlerV3()\n",
    "    MIXTRAL_TEMPLATE = \"[INST] {system} {user} [/INST]\"\n",
    "\n",
    "    # Get ids\n",
    "    all_ids_pickle = \"/root/geoarchive_paragraph_ids.pkl\"\n",
    "    with open(all_ids_pickle, \"rb\") as f:\n",
    "        all_ids = pickle.load(f)\n",
    "    batch_ids = all_ids[0:batch_size]\n",
    "\n",
    "    batch = []\n",
    "    for id in batch_ids:\n",
    "        paragraph = weaviate_client.data_object.get_by_id(id, class_name=\"Paragraph\")\n",
    "        text = paragraph[\"properties\"][\"text_content\"]\n",
    "        messages = prompt_handler.get_gpt_messages(text)\n",
    "        prompt = MIXTRAL_TEMPLATE.format(\n",
    "            system=messages[0][\"content\"], user=messages[1][\"content\"]\n",
    "        )\n",
    "        batch.append(prompt)\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(batch_size: int, llm: vllm.LLM) -> dict:\n",
    "    \"\"\"Benchmark the time required to process the llm inference.\"\"\"\n",
    "\n",
    "    sampling_params = vllm.SamplingParams(\n",
    "        temperature=0, max_tokens=2048, stop=[\"[/INST]\", \"[INST]\"]\n",
    "    )\n",
    "    prompts = get_batch(batch_size)\n",
    "\n",
    "    # Timed section\n",
    "    start_time = time.perf_counter()\n",
    "    outputs = llm.generate(prompts, sampling_params)\n",
    "    end_time = time.perf_counter()\n",
    "\n",
    "    # Calculate time per token\n",
    "    results = {}\n",
    "    results[\"total_time\"] = end_time - start_time\n",
    "    results[\"input_tokens\"] = sum([len(output.prompt_token_ids) for output in outputs])\n",
    "    results[\"output_tokens\"] = sum(\n",
    "        [len(output.outputs[0].token_ids) for output in outputs]\n",
    "    )\n",
    "    results[\"token_per_second\"] = (\n",
    "        results[\"input_tokens\"] + results[\"output_tokens\"]\n",
    "    ) / results[\"total_time\"]\n",
    "    results[\"outputs\"] = outputs\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for batch_size in [3]:\n",
    "    results[batch_size] = benchmark(batch_size, llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_size, r in results.items():\n",
    "    data = {k: v for k, v in r.items() if k != \"outputs\"}\n",
    "    print(f\"Batch size: {batch_size}, {data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tp: 2; eager: False, OOM at 50\n",
    "\n",
    "- Batch size: 1, {'total_time': 4.926733806729317, 'input_tokens': 369, 'output_tokens': 232, 'token_per_second': 121.98751212803651}\n",
    "- Batch size: 3, {'total_time': 15.417909558862448, 'input_tokens': 1122, 'output_tokens': 1098, 'token_per_second': 143.9883916509233}\n",
    "- Batch size: 5, {'total_time': 22.074966586660594, 'input_tokens': 1897, 'output_tokens': 2134, 'token_per_second': 182.60503290799284}\n",
    "- Batch size: 10, {'total_time': 26.61369479680434, 'input_tokens': 3807, 'output_tokens': 3494, 'token_per_second': 274.33244634926353}\n",
    "- Batch size: 20, {'total_time': 43.88761671539396, 'input_tokens': 7865, 'output_tokens': 7980, 'token_per_second': 361.03578152245}\n",
    "\n",
    "tp: 1; eager: True\n",
    "\n",
    "- Batch size: 1, {'total_time': 12.907626039814204, 'input_tokens': 369, 'output_tokens': 211, 'token_per_second': 44.93467646265562}\n",
    "- Batch size: 3, {'total_time': 52.06450440501794, 'input_tokens': 1122, 'output_tokens': 1161, 'token_per_second': 43.849452253308414}\n",
    "- Batch size: 5, {'total_time': 42.428753749933094, 'input_tokens': 1897, 'output_tokens': 2186, 'token_per_second': 96.23190971067442}\n",
    "- Batch size: 10, {'total_time': 49.54290740704164, 'input_tokens': 3807, 'output_tokens': 3833, 'token_per_second': 154.20976280681722}\n",
    "- Batch size: 20, {'total_time': 74.78418159391731, 'input_tokens': 7865, 'output_tokens': 6638, 'token_per_second': 193.9313861686978}\n",
    "- Batch size: 50, {'total_time': 172.57586133107543, 'input_tokens': 20854, 'output_tokens': 21304, 'token_per_second': 244.2867714802979}\n",
    "- Batch size: 100, {'total_time': 205.62514622602612, 'input_tokens': 40934, 'output_tokens': 40364, 'token_per_second': 395.36993160669203}\n",
    "- Batch size: 200, {'total_time': 293.219068787992, 'input_tokens': 82229, 'output_tokens': 74914, 'token_per_second': 535.923535428796}\n",
    "\n",
    "Summary:\n",
    "\n",
    "- Larger batch sizes enhance performance when not causing out-of-memory (OOM) issues.\n",
    "- Contrary to expectations, eager mode does not reduce processing speed per GPU.\n",
    "- I will adopt a batch size of 200 with a single GPU in eager mode to optimize queue management, checkpointing, and runtime."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
